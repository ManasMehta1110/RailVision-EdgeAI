{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e294c0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"[INFO] Using device:\", DEVICE)\n",
    "\n",
    "# =====================\n",
    "# PATHS\n",
    "# =====================\n",
    "DATA_ROOT = Path(\"../data/datasets\")\n",
    "\n",
    "RAILSEM_DIR = Path(\"../data/datasets/RailSem19/jpgs/rs19_val\")\n",
    "\n",
    "BDD_DIR     = DATA_ROOT / \"BDD100K/night_frames\"\n",
    "\n",
    "OUT_DIR = Path(\"../outputs/models/deblur\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOG_DIR = Path(\"../outputs/logs\")\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =====================\n",
    "# TRAIN PARAMS\n",
    "# =====================\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 4\n",
    "LR = 1e-4\n",
    "\n",
    "EPOCHS_STAGE1 = 60   # RailSem\n",
    "EPOCHS_STAGE2 = 40   # BDD Night\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2547c5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def motion_blur_kernel(size, angle):\n",
    "    kernel = np.zeros((size, size))\n",
    "    kernel[size // 2, :] = np.ones(size)\n",
    "    M = cv2.getRotationMatrix2D((size//2, size//2), angle, 1)\n",
    "    kernel = cv2.warpAffine(kernel, M, (size, size))\n",
    "    kernel /= kernel.sum()\n",
    "    return kernel\n",
    "\n",
    "def apply_motion_blur(img):\n",
    "    k = random.choice([7, 9, 11, 15])\n",
    "    angle = random.uniform(-15, 15)\n",
    "    kernel = motion_blur_kernel(k, angle)\n",
    "    return cv2.filter2D(img, -1, kernel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f910ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeblurDataset(Dataset):\n",
    "    def __init__(self, img_dir):\n",
    "        self.images = list(img_dir.glob(\"*.jpg\"))\n",
    "\n",
    "        if len(self.images) == 0:\n",
    "            raise RuntimeError(f\"No images found in {img_dir}\")\n",
    "\n",
    "        print(f\"[INFO] Loaded {len(self.images)} images from {img_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(str(self.images[idx]))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "        sharp = img.astype(np.float32) / 255.0\n",
    "        blur  = apply_motion_blur(img).astype(np.float32) / 255.0\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(blur).permute(2,0,1),\n",
    "            torch.from_numpy(sharp).permute(2,0,1)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53b1e479",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(c, c, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(c, c, 3, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv2(self.relu(self.conv1(x)))\n",
    "\n",
    "class DeblurNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.head = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.body = nn.Sequential(*[ResBlock(64) for _ in range(12)])\n",
    "        self.tail = nn.Conv2d(64, 3, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.head(x))\n",
    "        x = self.body(x)\n",
    "        return torch.clamp(self.tail(x), 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee8cb380",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    psnr_vals, ssim_vals = [], []\n",
    "\n",
    "    for blur, sharp in loader:\n",
    "        blur, sharp = blur.to(DEVICE), sharp.to(DEVICE)\n",
    "        out = model(blur)\n",
    "\n",
    "        out_np = out[0].permute(1,2,0).cpu().numpy()\n",
    "        gt_np  = sharp[0].permute(1,2,0).cpu().numpy()\n",
    "\n",
    "        psnr_vals.append(psnr(gt_np, out_np, data_range=1.0))\n",
    "        ssim_vals.append(\n",
    "            ssim(gt_np, out_np, channel_axis=2, data_range=1.0)\n",
    "        )\n",
    "\n",
    "    return np.mean(psnr_vals), np.mean(ssim_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77b70aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, best_psnr, path):\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"best_psnr\": best_psnr\n",
    "    }, path)\n",
    "\n",
    "def save_epoch(model, epoch, tag):\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        OUT_DIR / f\"{tag}_epoch_{epoch:03d}.pth\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4351b560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage(\n",
    "    model, train_loader, val_loader,\n",
    "    optimizer, start_epoch, epochs, tag\n",
    "):\n",
    "    best_psnr = -1\n",
    "\n",
    "    for epoch in range(start_epoch, start_epoch + epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for blur, sharp in tqdm(train_loader, desc=f\"{tag} Epoch {epoch}\"):\n",
    "            blur, sharp = blur.to(DEVICE), sharp.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(blur)\n",
    "            loss = nn.L1Loss()(out, sharp)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        val_psnr, val_ssim = evaluate(model, val_loader)\n",
    "\n",
    "        print(\n",
    "            f\"[{tag}] Epoch {epoch} | \"\n",
    "            f\"Loss={avg_loss:.4f} | \"\n",
    "            f\"PSNR={val_psnr:.2f} | \"\n",
    "            f\"SSIM={val_ssim:.4f}\"\n",
    "        )\n",
    "\n",
    "        save_epoch(model, epoch, tag)\n",
    "\n",
    "        if val_psnr > best_psnr:\n",
    "            best_psnr = val_psnr\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                OUT_DIR / f\"best_{tag}.pth\"\n",
    "            )\n",
    "\n",
    "        save_checkpoint(\n",
    "            model, optimizer, epoch, best_psnr,\n",
    "            OUT_DIR / f\"{tag}_checkpoint.pth\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d202e489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded 8500 images from ..\\data\\datasets\\RailSem19\\jpgs\\rs19_val\n",
      "[INFO] Loaded 8500 images from ..\\data\\datasets\\RailSem19\\jpgs\\rs19_val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "railsem Epoch 0:   0%|          | 0/2125 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "rail_train = DeblurDataset(RAILSEM_DIR)\n",
    "\n",
    "rail_loader = DataLoader(\n",
    "    rail_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "rail_val   = DeblurDataset(RAILSEM_DIR)\n",
    "rail_val_loader = DataLoader(\n",
    "    rail_val, batch_size=1,\n",
    "    shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "model = DeblurNet().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "train_stage(\n",
    "    model,\n",
    "    rail_loader,\n",
    "    rail_val_loader,\n",
    "    optimizer,\n",
    "    start_epoch=0,\n",
    "    epochs=EPOCHS_STAGE1,\n",
    "    tag=\"railsem\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1741158c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "No images found in ..\\data\\datasets\\BDD100K\\night_frames",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m bdd_train \u001b[38;5;241m=\u001b[39m \u001b[43mDeblurDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBDD_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m bdd_val   \u001b[38;5;241m=\u001b[39m DeblurDataset(BDD_DIR)\n\u001b[0;32m      4\u001b[0m bdd_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m      5\u001b[0m     bdd_train, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[0;32m      6\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      7\u001b[0m )\n",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m, in \u001b[0;36mDeblurDataset.__init__\u001b[1;34m(self, img_dir)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img_dir):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(img_dir\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(img_dir\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.png\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo images found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: No images found in ..\\data\\datasets\\BDD100K\\night_frames"
     ]
    }
   ],
   "source": [
    "bdd_train = DeblurDataset(BDD_DIR)\n",
    "bdd_val   = DeblurDataset(BDD_DIR)\n",
    "\n",
    "bdd_loader = DataLoader(\n",
    "    bdd_train, batch_size=BATCH_SIZE,\n",
    "    shuffle=True, num_workers=4, pin_memory=True\n",
    ")\n",
    "\n",
    "bdd_val_loader = DataLoader(\n",
    "    bdd_val, batch_size=1,\n",
    "    shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR * 0.5)\n",
    "\n",
    "train_stage(\n",
    "    model,\n",
    "    bdd_loader,\n",
    "    bdd_val_loader,\n",
    "    optimizer,\n",
    "    start_epoch=EPOCHS_STAGE1,\n",
    "    epochs=EPOCHS_STAGE2,\n",
    "    tag=\"bdd_night\"\n",
    ")\n",
    "\n",
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    OUT_DIR / \"best_model_deblur.pth\"\n",
    ")\n",
    "\n",
    "print(\"✅ Final deblur model saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8f95c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "No images in ..\\data\\datasets\\RailSem19\\images_train",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 97\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(P)), \u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(S))\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# ---------------- LOADERS ----------------\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mDeblurDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m val_ds   \u001b[38;5;241m=\u001b[39m DeblurDataset(VAL_DIR)\n\u001b[0;32m    100\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_ds, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 47\u001b[0m, in \u001b[0;36mDeblurDataset.__init__\u001b[1;34m(self, folder)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, folder):\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(folder\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(folder\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.png\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo images in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: No images in ..\\data\\datasets\\RailSem19\\images_train"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SINGLE CELL – COMPLETE DEBLUR TRAINING WITH PSNR & SSIM\n",
    "# ============================================================\n",
    "\n",
    "import cv2, torch, random, numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"[INFO] Using device:\", DEVICE)\n",
    "\n",
    "DATA_ROOT = Path(\"../data/datasets/RailSem19\")\n",
    "TRAIN_DIR = DATA_ROOT / \"images_train\"\n",
    "VAL_DIR   = DATA_ROOT / \"images_val\"\n",
    "\n",
    "OUT_DIR = Path(\"../outputs/models/deblur\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 4\n",
    "LR = 1e-4\n",
    "EPOCHS = 50\n",
    "\n",
    "# ---------------- MOTION BLUR ----------------\n",
    "def motion_blur_kernel(size, angle):\n",
    "    k = np.zeros((size, size))\n",
    "    k[size // 2] = np.ones(size)\n",
    "    M = cv2.getRotationMatrix2D((size//2, size//2), angle, 1)\n",
    "    k = cv2.warpAffine(k, M, (size, size))\n",
    "    return k / k.sum()\n",
    "\n",
    "def apply_motion_blur(img):\n",
    "    ksize = random.choice([7, 9, 11, 15])\n",
    "    angle = random.uniform(-15, 15)\n",
    "    return cv2.filter2D(img, -1, motion_blur_kernel(ksize, angle))\n",
    "\n",
    "# ---------------- DATASET ----------------\n",
    "class DeblurDataset(Dataset):\n",
    "    def __init__(self, folder):\n",
    "        self.files = list(folder.glob(\"*.jpg\")) + list(folder.glob(\"*.png\"))\n",
    "        assert len(self.files) > 0, f\"No images in {folder}\"\n",
    "\n",
    "    def __len__(self): return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(str(self.files[idx]))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "        sharp = img.astype(np.float32) / 255.0\n",
    "        blur  = apply_motion_blur(img).astype(np.float32) / 255.0\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(blur).permute(2,0,1),\n",
    "            torch.from_numpy(sharp).permute(2,0,1)\n",
    "        )\n",
    "\n",
    "# ---------------- MODEL ----------------\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv2d(c, c, 3, padding=1)\n",
    "        self.c2 = nn.Conv2d(c, c, 3, padding=1)\n",
    "        self.r  = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        return x + self.c2(self.r(self.c1(x)))\n",
    "\n",
    "class DeblurNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.b = nn.Sequential(*[ResBlock(64) for _ in range(12)])\n",
    "        self.t = nn.Conv2d(64, 3, 3, padding=1)\n",
    "    def forward(self, x):\n",
    "        return torch.clamp(self.t(self.b(torch.relu(self.h(x)))), 0, 1)\n",
    "\n",
    "# ---------------- METRICS ----------------\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    P, S = [], []\n",
    "    for b, s in loader:\n",
    "        b, s = b.to(DEVICE), s.to(DEVICE)\n",
    "        o = model(b)[0].permute(1,2,0).cpu().numpy()\n",
    "        s = s[0].permute(1,2,0).cpu().numpy()\n",
    "        P.append(psnr(s, o, data_range=1.0))\n",
    "        S.append(ssim(s, o, channel_axis=2, data_range=1.0))\n",
    "    return float(np.mean(P)), float(np.mean(S))\n",
    "\n",
    "# ---------------- LOADERS ----------------\n",
    "train_ds = DeblurDataset(TRAIN_DIR)\n",
    "val_ds   = DeblurDataset(VAL_DIR)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader   = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=2)\n",
    "\n",
    "# ---------------- TRAIN ----------------\n",
    "model = DeblurNet().to(DEVICE)\n",
    "opt = optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.L1Loss()\n",
    "best_psnr = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "\n",
    "    for blur, sharp in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "        blur, sharp = blur.to(DEVICE), sharp.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        out = model(blur)\n",
    "        loss = loss_fn(out, sharp)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "    loss_avg = loss_sum / len(train_loader)\n",
    "    v_psnr, v_ssim = evaluate(model, val_loader)\n",
    "\n",
    "    print(\n",
    "        f\"[Epoch {epoch}] \"\n",
    "        f\"TrainLoss={loss_avg:.4f} | \"\n",
    "        f\"PSNR={v_psnr:.2f} dB | \"\n",
    "        f\"SSIM={v_ssim:.4f}\"\n",
    "    )\n",
    "\n",
    "    torch.save(\n",
    "        {\"epoch\": epoch, \"model\": model.state_dict(),\n",
    "         \"loss\": loss_avg, \"psnr\": v_psnr, \"ssim\": v_ssim},\n",
    "        OUT_DIR / f\"epoch_{epoch}.pth\"\n",
    "    )\n",
    "\n",
    "    if v_psnr > best_psnr:\n",
    "        best_psnr = v_psnr\n",
    "        torch.save({\"model\": model.state_dict()},\n",
    "                   OUT_DIR / \"best_model_deblur.pth\")\n",
    "\n",
    "print(\"✅ Training complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "573e9641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "No images found in ..\\data\\datasets\\RailSem19\\jpgs\\rs19",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 98\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(P)), \u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(S))\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# ---------------- LOADERS ----------------\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mRailSemDeblurDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m val_ds   \u001b[38;5;241m=\u001b[39m RailSemDeblurDataset(VAL_DIR)\n\u001b[0;32m    101\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_ds, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 47\u001b[0m, in \u001b[0;36mRailSemDeblurDataset.__init__\u001b[1;34m(self, folder)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, folder):\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(folder\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo images found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: No images found in ..\\data\\datasets\\RailSem19\\jpgs\\rs19"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# RAILSEM19 DEBLUR TRAINING – CORRECT STRUCTURE – ONE CELL\n",
    "# ============================================================\n",
    "\n",
    "import cv2, torch, random, numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"[INFO] Using device:\", DEVICE)\n",
    "\n",
    "DATA_ROOT = Path(\"../data/datasets/RailSem19/jpgs\")\n",
    "TRAIN_DIR = DATA_ROOT / \"rs19\"\n",
    "VAL_DIR   = DATA_ROOT / \"rs19_val\"\n",
    "\n",
    "OUT_DIR = Path(\"../outputs/models/deblur\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 4\n",
    "LR = 1e-4\n",
    "EPOCHS = 40\n",
    "\n",
    "# ---------------- MOTION BLUR ----------------\n",
    "def motion_blur_kernel(size, angle):\n",
    "    kernel = np.zeros((size, size))\n",
    "    kernel[size // 2] = np.ones(size)\n",
    "    M = cv2.getRotationMatrix2D((size//2, size//2), angle, 1)\n",
    "    kernel = cv2.warpAffine(kernel, M, (size, size))\n",
    "    return kernel / kernel.sum()\n",
    "\n",
    "def apply_motion_blur(img):\n",
    "    k = random.choice([9, 11, 15])\n",
    "    a = random.uniform(-20, 20)\n",
    "    return cv2.filter2D(img, -1, motion_blur_kernel(k, a))\n",
    "\n",
    "# ---------------- DATASET ----------------\n",
    "class RailSemDeblurDataset(Dataset):\n",
    "    def __init__(self, folder):\n",
    "        self.files = sorted(list(folder.glob(\"*.jpg\")))\n",
    "        assert len(self.files) > 0, f\"No images found in {folder}\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(str(self.files[idx]))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "        sharp = img.astype(np.float32) / 255.0\n",
    "        blur  = apply_motion_blur(img).astype(np.float32) / 255.0\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(blur).permute(2,0,1),\n",
    "            torch.from_numpy(sharp).permute(2,0,1)\n",
    "        )\n",
    "\n",
    "# ---------------- MODEL ----------------\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv2d(c, c, 3, padding=1)\n",
    "        self.c2 = nn.Conv2d(c, c, 3, padding=1)\n",
    "        self.r  = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        return x + self.c2(self.r(self.c1(x)))\n",
    "\n",
    "class DeblurNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.b = nn.Sequential(*[ResBlock(64) for _ in range(12)])\n",
    "        self.t = nn.Conv2d(64, 3, 3, padding=1)\n",
    "    def forward(self, x):\n",
    "        return torch.clamp(self.t(self.b(torch.relu(self.h(x)))), 0, 1)\n",
    "\n",
    "# ---------------- METRICS ----------------\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    P, S = [], []\n",
    "    for b, s in loader:\n",
    "        b, s = b.to(DEVICE), s.to(DEVICE)\n",
    "        o = model(b)[0].permute(1,2,0).cpu().numpy()\n",
    "        s = s[0].permute(1,2,0).cpu().numpy()\n",
    "        P.append(psnr(s, o, data_range=1.0))\n",
    "        S.append(ssim(s, o, channel_axis=2, data_range=1.0))\n",
    "    return float(np.mean(P)), float(np.mean(S))\n",
    "\n",
    "# ---------------- LOADERS ----------------\n",
    "train_ds = RailSemDeblurDataset(TRAIN_DIR)\n",
    "val_ds   = RailSemDeblurDataset(VAL_DIR)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader   = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"[INFO] Train images: {len(train_ds)}\")\n",
    "print(f\"[INFO] Val images:   {len(val_ds)}\")\n",
    "\n",
    "# ---------------- TRAIN ----------------\n",
    "model = DeblurNet().to(DEVICE)\n",
    "opt = optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.L1Loss()\n",
    "best_psnr = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "\n",
    "    for blur, sharp in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "        blur, sharp = blur.to(DEVICE), sharp.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        out = model(blur)\n",
    "        loss = loss_fn(out, sharp)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "    loss_avg = loss_sum / len(train_loader)\n",
    "    v_psnr, v_ssim = evaluate(model, val_loader)\n",
    "\n",
    "    print(\n",
    "        f\"[Epoch {epoch}] \"\n",
    "        f\"TrainLoss={loss_avg:.4f} | \"\n",
    "        f\"PSNR={v_psnr:.2f} dB | \"\n",
    "        f\"SSIM={v_ssim:.4f}\"\n",
    "    )\n",
    "\n",
    "    torch.save(\n",
    "        {\"epoch\": epoch, \"model\": model.state_dict(),\n",
    "         \"loss\": loss_avg, \"psnr\": v_psnr, \"ssim\": v_ssim},\n",
    "        OUT_DIR / f\"epoch_{epoch}.pth\"\n",
    "    )\n",
    "\n",
    "    if v_psnr > best_psnr:\n",
    "        best_psnr = v_psnr\n",
    "        torch.save({\"model\": model.state_dict()},\n",
    "                   OUT_DIR / \"best_model_deblur.pth\")\n",
    "\n",
    "print(\"✅ RailSem19 Deblur training finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d094f328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n",
      "[INFO] Total images: 8500\n",
      "[INFO] Train images: 7650\n",
      "[INFO] Val images:   850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1913 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# RAILSEM19 DEBLUR TRAINING – AUTO SPLIT (YOUR STRUCTURE)\n",
    "# ============================================================\n",
    "\n",
    "import cv2, torch, random, numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"[INFO] Using device:\", DEVICE)\n",
    "\n",
    "DATA_DIR = Path(\"../data/datasets/RailSem19/jpgs/rs19_val\")\n",
    "assert DATA_DIR.exists(), f\"Folder missing: {DATA_DIR}\"\n",
    "\n",
    "OUT_DIR = Path(\"../outputs/models/deblur\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 4\n",
    "LR = 1e-4\n",
    "EPOCHS = 40\n",
    "TRAIN_SPLIT = 0.9\n",
    "\n",
    "# ---------------- MOTION BLUR ----------------\n",
    "def motion_blur_kernel(size, angle):\n",
    "    k = np.zeros((size, size))\n",
    "    k[size//2] = np.ones(size)\n",
    "    M = cv2.getRotationMatrix2D((size//2, size//2), angle, 1)\n",
    "    k = cv2.warpAffine(k, M, (size, size))\n",
    "    return k / k.sum()\n",
    "\n",
    "def apply_motion_blur(img):\n",
    "    k = random.choice([9, 11, 15])\n",
    "    a = random.uniform(-20, 20)\n",
    "    return cv2.filter2D(img, -1, motion_blur_kernel(k, a))\n",
    "\n",
    "# ---------------- DATASET ----------------\n",
    "class RailSemDeblurDataset(Dataset):\n",
    "    def __init__(self, folder):\n",
    "        self.files = sorted(list(folder.glob(\"*.jpg\")))\n",
    "        assert len(self.files) > 0, f\"No images found in {folder}\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(str(self.files[idx]))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "        sharp = img.astype(np.float32) / 255.0\n",
    "        blur  = apply_motion_blur(img).astype(np.float32) / 255.0\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(blur).permute(2,0,1),\n",
    "            torch.from_numpy(sharp).permute(2,0,1)\n",
    "        )\n",
    "\n",
    "# ---------------- MODEL ----------------\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv2d(c, c, 3, padding=1)\n",
    "        self.c2 = nn.Conv2d(c, c, 3, padding=1)\n",
    "        self.r  = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        return x + self.c2(self.r(self.c1(x)))\n",
    "\n",
    "class DeblurNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.b = nn.Sequential(*[ResBlock(64) for _ in range(12)])\n",
    "        self.t = nn.Conv2d(64, 3, 3, padding=1)\n",
    "    def forward(self, x):\n",
    "        return torch.clamp(self.t(self.b(torch.relu(self.h(x)))), 0, 1)\n",
    "\n",
    "# ---------------- METRICS ----------------\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    P, S = [], []\n",
    "    for b, s in loader:\n",
    "        b, s = b.to(DEVICE), s.to(DEVICE)\n",
    "        o = model(b)[0].permute(1,2,0).cpu().numpy()\n",
    "        s = s[0].permute(1,2,0).cpu().numpy()\n",
    "        P.append(psnr(s, o, data_range=1.0))\n",
    "        S.append(ssim(s, o, channel_axis=2, data_range=1.0))\n",
    "    return float(np.mean(P)), float(np.mean(S))\n",
    "\n",
    "# ---------------- SPLIT ----------------\n",
    "full_ds = RailSemDeblurDataset(DATA_DIR)\n",
    "n_train = int(len(full_ds) * TRAIN_SPLIT)\n",
    "n_val   = len(full_ds) - n_train\n",
    "train_ds, val_ds = random_split(full_ds, [n_train, n_val])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader   = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"[INFO] Total images: {len(full_ds)}\")\n",
    "print(f\"[INFO] Train images: {len(train_ds)}\")\n",
    "print(f\"[INFO] Val images:   {len(val_ds)}\")\n",
    "\n",
    "# ---------------- TRAIN ----------------\n",
    "model = DeblurNet().to(DEVICE)\n",
    "opt = optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.L1Loss()\n",
    "best_psnr = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "\n",
    "    for blur, sharp in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "        blur, sharp = blur.to(DEVICE), sharp.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        out = model(blur)\n",
    "        loss = loss_fn(out, sharp)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "    loss_avg = loss_sum / len(train_loader)\n",
    "    v_psnr, v_ssim = evaluate(model, val_loader)\n",
    "\n",
    "    print(\n",
    "        f\"[Epoch {epoch}] \"\n",
    "        f\"TrainLoss={loss_avg:.4f} | \"\n",
    "        f\"PSNR={v_psnr:.2f} dB | \"\n",
    "        f\"SSIM={v_ssim:.4f}\"\n",
    "    )\n",
    "\n",
    "    torch.save(\n",
    "        {\"epoch\": epoch, \"model\": model.state_dict(),\n",
    "         \"loss\": loss_avg, \"psnr\": v_psnr, \"ssim\": v_ssim},\n",
    "        OUT_DIR / f\"epoch_{epoch}.pth\"\n",
    "    )\n",
    "\n",
    "    if v_psnr > best_psnr:\n",
    "        best_psnr = v_psnr\n",
    "        torch.save({\"model\": model.state_dict()},\n",
    "                   OUT_DIR / \"best_model_deblur.pth\")\n",
    "\n",
    "print(\"✅ RailSem19 Deblur training finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10bee7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n",
      "[DATASET] Found 8500 images\n",
      "[INFO] Total: 8500  |  Train: 7650  |  Val: 850\n",
      "[INFO] Batch size: 1  |  Workers: 0  |  Pin memory: True\n",
      "\n",
      "Testing first batch ... OK\n",
      "   shapes → blur: torch.Size([1, 3, 256, 256])   sharp: torch.Size([1, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   1%|▏         | 101/7650 [00:10<12:18, 10.22it/s, loss=0.0449]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch  100] loss=0.0433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   3%|▎         | 200/7650 [00:19<10:37, 11.69it/s, loss=0.0360]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch  200] loss=0.0400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   4%|▍         | 301/7650 [00:31<13:32,  9.04it/s, loss=0.0291]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch  300] loss=0.0459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   5%|▌         | 401/7650 [00:40<10:40, 11.32it/s, loss=0.0319]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch  400] loss=0.0457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   7%|▋         | 500/7650 [00:50<11:17, 10.56it/s, loss=0.0415]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch  500] loss=0.0331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   8%|▊         | 600/7650 [00:59<10:39, 11.03it/s, loss=0.0319]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch  600] loss=0.0325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   9%|▉         | 702/7650 [01:09<10:52, 10.65it/s, loss=0.0344]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch  700] loss=0.0314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  10%|█         | 800/7650 [01:19<10:20, 11.05it/s, loss=0.0211]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch  800] loss=0.0486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  12%|█▏        | 901/7650 [01:29<10:43, 10.49it/s, loss=0.0418]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch  900] loss=0.0323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  13%|█▎        | 1000/7650 [01:38<09:54, 11.19it/s, loss=0.0481]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 1000] loss=0.0476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  14%|█▍        | 1100/7650 [01:48<10:46, 10.13it/s, loss=0.0774]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 1100] loss=0.0390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  16%|█▌        | 1201/7650 [01:58<10:14, 10.49it/s, loss=0.0297]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 1200] loss=0.0656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  17%|█▋        | 1302/7650 [02:08<10:49,  9.78it/s, loss=0.0358]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 1300] loss=0.0258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  18%|█▊        | 1401/7650 [02:18<09:53, 10.52it/s, loss=0.0198]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 1400] loss=0.0187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  20%|█▉        | 1501/7650 [02:27<12:38,  8.10it/s, loss=0.0488]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 1500] loss=0.0317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  21%|██        | 1601/7650 [02:38<10:31,  9.57it/s, loss=0.0298]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 1600] loss=0.0722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  22%|██▏       | 1701/7650 [02:47<09:19, 10.64it/s, loss=0.0406]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 1700] loss=0.0458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  24%|██▎       | 1801/7650 [02:56<08:36, 11.33it/s, loss=0.0292]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 1800] loss=0.0333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  25%|██▍       | 1902/7650 [03:07<08:32, 11.23it/s, loss=0.0290]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 1900] loss=0.0257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  26%|██▌       | 1999/7650 [03:16<08:17, 11.36it/s, loss=0.0719]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 2000] loss=0.0719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  27%|██▋       | 2102/7650 [03:25<08:13, 11.24it/s, loss=0.0404]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 2100] loss=0.0348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  29%|██▉       | 2200/7650 [03:35<09:37,  9.44it/s, loss=0.0322]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 2200] loss=0.0370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  30%|███       | 2300/7650 [03:45<08:08, 10.96it/s, loss=0.0189]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 2300] loss=0.0430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  31%|███▏      | 2401/7650 [03:54<11:00,  7.95it/s, loss=0.0448]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 2400] loss=0.0349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  33%|███▎      | 2502/7650 [04:05<07:44, 11.08it/s, loss=0.0300]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 2500] loss=0.0307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  34%|███▍      | 2601/7650 [04:14<07:31, 11.19it/s, loss=0.0392]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 2600] loss=0.0155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  35%|███▌      | 2700/7650 [04:24<07:15, 11.36it/s, loss=0.0342]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 2700] loss=0.0150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  37%|███▋      | 2800/7650 [04:34<07:37, 10.59it/s, loss=0.0284]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 2800] loss=0.0475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  38%|███▊      | 2899/7650 [04:44<07:16, 10.87it/s, loss=0.0417]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 2900] loss=0.0417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  39%|███▉      | 3001/7650 [04:54<06:55, 11.20it/s, loss=0.0238]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 3000] loss=0.0124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  41%|████      | 3102/7650 [05:04<07:06, 10.65it/s, loss=0.0388]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 3100] loss=0.0312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  42%|████▏     | 3200/7650 [05:14<06:53, 10.76it/s, loss=0.0199]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 3200] loss=0.0336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  43%|████▎     | 3302/7650 [05:25<07:57,  9.11it/s, loss=0.0193]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 3300] loss=0.0434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  44%|████▍     | 3400/7650 [05:37<07:56,  8.92it/s, loss=0.0279]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 3400] loss=0.0196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  46%|████▌     | 3501/7650 [05:48<07:23,  9.36it/s, loss=0.0514]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 3500] loss=0.0462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  47%|████▋     | 3601/7650 [05:59<07:21,  9.17it/s, loss=0.0257]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 3600] loss=0.0305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  48%|████▊     | 3701/7650 [06:10<06:26, 10.22it/s, loss=0.0252]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 3700] loss=0.0253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  50%|████▉     | 3800/7650 [06:21<05:52, 10.93it/s, loss=0.0370]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 3800] loss=0.0093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  51%|█████     | 3901/7650 [06:34<07:16,  8.58it/s, loss=0.0190]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 3900] loss=0.0387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  52%|█████▏    | 4001/7650 [06:45<07:00,  8.67it/s, loss=0.0326]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 4000] loss=0.0295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  54%|█████▎    | 4100/7650 [06:56<06:48,  8.69it/s, loss=0.0464]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 4100] loss=0.0464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  55%|█████▍    | 4201/7650 [07:09<08:01,  7.17it/s, loss=0.0315]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 4200] loss=0.0237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  56%|█████▌    | 4301/7650 [07:20<05:54,  9.46it/s, loss=0.0338]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 4300] loss=0.0263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  58%|█████▊    | 4400/7650 [07:32<05:17, 10.22it/s, loss=0.0387]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 4400] loss=0.0367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  59%|█████▉    | 4502/7650 [07:42<05:11, 10.10it/s, loss=0.0383]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 4500] loss=0.0354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  60%|██████    | 4601/7650 [07:52<04:40, 10.88it/s, loss=0.0308]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 4600] loss=0.0278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  61%|██████▏   | 4702/7650 [08:02<04:38, 10.59it/s, loss=0.0427]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 4700] loss=0.0485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  63%|██████▎   | 4801/7650 [08:12<05:12,  9.12it/s, loss=0.0395]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 4800] loss=0.0441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  64%|██████▍   | 4901/7650 [08:23<05:16,  8.68it/s, loss=0.0489]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 4900] loss=0.0250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  65%|██████▌   | 5001/7650 [08:33<04:15, 10.36it/s, loss=0.0311]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 5000] loss=0.0384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  67%|██████▋   | 5101/7650 [08:44<04:26,  9.56it/s, loss=0.0227]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 5100] loss=0.0308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  68%|██████▊   | 5202/7650 [08:55<03:40, 11.09it/s, loss=0.0349]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 5200] loss=0.0291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  69%|██████▉   | 5301/7650 [09:05<03:48, 10.27it/s, loss=0.0324]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 5300] loss=0.0335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  71%|███████   | 5400/7650 [09:16<03:30, 10.69it/s, loss=0.0386]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 5400] loss=0.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  72%|███████▏  | 5500/7650 [09:25<03:09, 11.36it/s, loss=0.0273]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 5500] loss=0.0508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  73%|███████▎  | 5602/7650 [09:36<03:27,  9.87it/s, loss=0.0337]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 5600] loss=0.0373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  75%|███████▍  | 5701/7650 [09:47<03:02, 10.68it/s, loss=0.0385]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 5700] loss=0.0335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  76%|███████▌  | 5801/7650 [09:57<03:09,  9.75it/s, loss=0.0227]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 5800] loss=0.0231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  77%|███████▋  | 5901/7650 [10:09<03:00,  9.68it/s, loss=0.0263]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 5900] loss=0.0351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  78%|███████▊  | 6001/7650 [10:19<03:11,  8.60it/s, loss=0.0286]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 6000] loss=0.0509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  80%|███████▉  | 6102/7650 [10:30<02:29, 10.35it/s, loss=0.0151]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 6100] loss=0.0354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  81%|████████  | 6201/7650 [10:41<02:34,  9.36it/s, loss=0.0383]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 6200] loss=0.0295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  82%|████████▏ | 6300/7650 [10:51<02:36,  8.63it/s, loss=0.0294]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 6300] loss=0.0384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  84%|████████▎ | 6400/7650 [11:02<02:18,  9.01it/s, loss=0.0106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 6400] loss=0.0106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  85%|████████▍ | 6502/7650 [11:12<01:48, 10.60it/s, loss=0.0210]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 6500] loss=0.0145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  86%|████████▋ | 6599/7650 [11:22<01:41, 10.36it/s, loss=0.0257]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 6600] loss=0.0257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  88%|████████▊ | 6700/7650 [11:33<01:44,  9.06it/s, loss=0.0308]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 6700] loss=0.0366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  89%|████████▉ | 6801/7650 [11:44<01:21, 10.36it/s, loss=0.0227]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 6800] loss=0.0309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  90%|█████████ | 6901/7650 [11:54<01:10, 10.58it/s, loss=0.0405]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 6900] loss=0.0361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  92%|█████████▏| 7000/7650 [12:04<01:00, 10.79it/s, loss=0.0245]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 7000] loss=0.0381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  93%|█████████▎| 7102/7650 [12:14<00:55,  9.84it/s, loss=0.0472]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 7100] loss=0.0312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  94%|█████████▍| 7201/7650 [12:24<00:53,  8.40it/s, loss=0.0274]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 7200] loss=0.0419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  95%|█████████▌| 7301/7650 [12:34<00:34, 10.14it/s, loss=0.0349]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 7300] loss=0.0454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  97%|█████████▋| 7402/7650 [12:45<00:24, 10.22it/s, loss=0.0290]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 7400] loss=0.0335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  98%|█████████▊| 7501/7650 [12:56<00:15,  9.32it/s, loss=0.0225]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 7500] loss=0.0226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  99%|█████████▉| 7600/7650 [13:06<00:05,  9.48it/s, loss=0.0164]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 7600] loss=0.0244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 7650/7650 [13:11<00:00,  9.67it/s, loss=0.0265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1] Loss=0.0365 | PSNR=25.95 dB | SSIM=0.7306\n",
      "  → New best model saved (PSNR 25.95)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:   1%|▏         | 101/7650 [00:09<12:09, 10.34it/s, loss=0.0118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch  100] loss=0.0293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:   3%|▎         | 201/7650 [00:18<10:48, 11.49it/s, loss=0.0321]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch  200] loss=0.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:   4%|▍         | 301/7650 [00:28<14:12,  8.62it/s, loss=0.0246]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch  300] loss=0.0343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:   5%|▌         | 402/7650 [00:38<10:52, 11.11it/s, loss=0.0272]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch  400] loss=0.0414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:   7%|▋         | 500/7650 [00:47<10:22, 11.49it/s, loss=0.0721]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch  500] loss=0.0404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:   8%|▊         | 599/7650 [00:57<12:28,  9.41it/s, loss=0.0204]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch  600] loss=0.0204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:   9%|▉         | 701/7650 [01:07<09:54, 11.69it/s, loss=0.0230]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch  700] loss=0.0319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  10%|█         | 801/7650 [01:16<10:22, 11.00it/s, loss=0.0243]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch  800] loss=0.0065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  12%|█▏        | 899/7650 [01:26<11:23,  9.88it/s, loss=0.0391]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch  900] loss=0.0391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  13%|█▎        | 1000/7650 [01:37<11:58,  9.26it/s, loss=0.0317]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 1000] loss=0.0520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  14%|█▍        | 1100/7650 [01:46<09:37, 11.34it/s, loss=0.0314]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 1100] loss=0.0328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  16%|█▌        | 1201/7650 [01:56<10:38, 10.10it/s, loss=0.0313]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 1200] loss=0.0446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  17%|█▋        | 1301/7650 [02:06<10:51,  9.75it/s, loss=0.0242]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 1300] loss=0.0178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  18%|█▊        | 1401/7650 [02:16<12:01,  8.66it/s, loss=0.0268]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 1400] loss=0.0415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  20%|█▉        | 1501/7650 [02:26<13:23,  7.65it/s, loss=0.0447]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 1500] loss=0.0733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  21%|██        | 1600/7650 [02:37<10:18,  9.78it/s, loss=0.0426]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 1600] loss=0.0258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  22%|██▏       | 1700/7650 [02:46<08:50, 11.22it/s, loss=0.0452]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 1700] loss=0.0407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  24%|██▎       | 1801/7650 [02:55<08:33, 11.39it/s, loss=0.0174]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 1800] loss=0.0234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  25%|██▍       | 1900/7650 [03:05<08:25, 11.37it/s, loss=0.0510]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 1900] loss=0.0196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  26%|██▌       | 2002/7650 [03:15<08:52, 10.61it/s, loss=0.0367]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 2000] loss=0.0382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  27%|██▋       | 2102/7650 [03:25<08:04, 11.45it/s, loss=0.0439]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 2100] loss=0.0282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  29%|██▉       | 2202/7650 [03:35<08:58, 10.12it/s, loss=0.0207]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 2200] loss=0.0503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  30%|███       | 2300/7650 [03:45<08:25, 10.58it/s, loss=0.0372]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 2300] loss=0.0261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  31%|███▏      | 2400/7650 [03:55<08:40, 10.08it/s, loss=0.0226]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 2400] loss=0.0232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  33%|███▎      | 2502/7650 [04:04<07:37, 11.25it/s, loss=0.0468]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 2500] loss=0.0568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  34%|███▍      | 2602/7650 [04:14<08:27,  9.95it/s, loss=0.0223]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 2600] loss=0.0554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  35%|███▌      | 2700/7650 [04:24<07:38, 10.79it/s, loss=0.0150]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 2700] loss=0.0407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  37%|███▋      | 2800/7650 [04:34<07:50, 10.31it/s, loss=0.0132]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 2800] loss=0.0435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  38%|███▊      | 2902/7650 [04:44<07:30, 10.54it/s, loss=0.0390]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 2900] loss=0.0502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  39%|███▉      | 3002/7650 [04:54<07:29, 10.34it/s, loss=0.0392]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 3000] loss=0.0327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  41%|████      | 3100/7650 [05:04<07:57,  9.53it/s, loss=0.0386]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 3100] loss=0.0206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  42%|████▏     | 3200/7650 [05:13<07:06, 10.44it/s, loss=0.0332]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 3200] loss=0.0237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  43%|████▎     | 3301/7650 [05:23<06:23, 11.34it/s, loss=0.0377]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 3300] loss=0.0195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  44%|████▍     | 3401/7650 [05:33<06:44, 10.50it/s, loss=0.0307]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 3400] loss=0.0234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  46%|████▌     | 3501/7650 [05:43<06:24, 10.78it/s, loss=0.0233]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 3500] loss=0.0507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  47%|████▋     | 3602/7650 [05:52<06:22, 10.57it/s, loss=0.0227]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 3600] loss=0.0339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  48%|████▊     | 3701/7650 [06:02<06:11, 10.63it/s, loss=0.0347]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 3700] loss=0.0373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  50%|████▉     | 3801/7650 [06:11<05:52, 10.91it/s, loss=0.0257]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 3800] loss=0.0150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  51%|█████     | 3901/7650 [06:20<05:29, 11.36it/s, loss=0.0147]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 3900] loss=0.0221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  52%|█████▏    | 4001/7650 [06:31<05:57, 10.22it/s, loss=0.0355]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 4000] loss=0.0376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  54%|█████▎    | 4102/7650 [06:41<05:06, 11.57it/s, loss=0.0139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 4100] loss=0.0496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  55%|█████▍    | 4201/7650 [06:50<05:11, 11.09it/s, loss=0.0395]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 4200] loss=0.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  56%|█████▌    | 4302/7650 [07:01<04:49, 11.58it/s, loss=0.0302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 4300] loss=0.0333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  58%|█████▊    | 4402/7650 [07:10<04:57, 10.91it/s, loss=0.0334]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 4400] loss=0.0242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  59%|█████▉    | 4502/7650 [07:20<04:48, 10.89it/s, loss=0.0251]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 4500] loss=0.0329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  60%|██████    | 4601/7650 [07:30<04:59, 10.18it/s, loss=0.0264]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 4600] loss=0.0242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  61%|██████▏   | 4702/7650 [07:39<04:22, 11.22it/s, loss=0.0355]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 4700] loss=0.0406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  63%|██████▎   | 4801/7650 [07:49<04:34, 10.37it/s, loss=0.0423]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 4800] loss=0.0329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  64%|██████▍   | 4901/7650 [07:58<04:43,  9.71it/s, loss=0.0229]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 4900] loss=0.0240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  65%|██████▌   | 5000/7650 [08:08<04:24, 10.00it/s, loss=0.0064]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 5000] loss=0.0136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  67%|██████▋   | 5102/7650 [08:18<04:00, 10.61it/s, loss=0.0215]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 5100] loss=0.0387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  68%|██████▊   | 5202/7650 [08:27<04:14,  9.61it/s, loss=0.0398]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 5200] loss=0.0232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  69%|██████▉   | 5300/7650 [08:37<03:32, 11.06it/s, loss=0.0137]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 5300] loss=0.0444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  71%|███████   | 5401/7650 [08:47<03:26, 10.90it/s, loss=0.0253]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 5400] loss=0.0380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  72%|███████▏  | 5501/7650 [08:56<03:21, 10.65it/s, loss=0.0569]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 5500] loss=0.0455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  73%|███████▎  | 5601/7650 [09:06<03:23, 10.07it/s, loss=0.0375]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 5600] loss=0.0320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  75%|███████▍  | 5701/7650 [09:16<02:57, 11.00it/s, loss=0.0281]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 5700] loss=0.0368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  76%|███████▌  | 5802/7650 [09:25<02:52, 10.68it/s, loss=0.0203]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 5800] loss=0.0357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  77%|███████▋  | 5902/7650 [09:35<02:36, 11.19it/s, loss=0.0353]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 5900] loss=0.0586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  78%|███████▊  | 6001/7650 [09:44<02:36, 10.57it/s, loss=0.0273]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 6000] loss=0.0216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  80%|███████▉  | 6102/7650 [09:54<02:19, 11.13it/s, loss=0.0484]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 6100] loss=0.0418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  81%|████████  | 6202/7650 [10:03<02:03, 11.77it/s, loss=0.0097]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 6200] loss=0.0288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  82%|████████▏ | 6302/7650 [10:13<01:57, 11.47it/s, loss=0.0219]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 6300] loss=0.0212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  84%|████████▎ | 6401/7650 [10:22<02:12,  9.40it/s, loss=0.0223]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 6400] loss=0.0174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  85%|████████▍ | 6500/7650 [10:33<02:13,  8.61it/s, loss=0.0232]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 6500] loss=0.0442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  86%|████████▋ | 6601/7650 [10:44<01:41, 10.33it/s, loss=0.0201]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 6600] loss=0.0307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  88%|████████▊ | 6702/7650 [10:54<01:25, 11.06it/s, loss=0.0218]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 6700] loss=0.0250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  89%|████████▉ | 6801/7650 [11:04<01:33,  9.13it/s, loss=0.0117]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 6800] loss=0.0224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  90%|█████████ | 6899/7650 [11:14<01:10, 10.69it/s, loss=0.0454]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 6900] loss=0.0454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  92%|█████████▏| 7000/7650 [11:25<01:06,  9.83it/s, loss=0.0270]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 7000] loss=0.0204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  93%|█████████▎| 7101/7650 [11:35<00:56,  9.74it/s, loss=0.0296]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 7100] loss=0.0185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  94%|█████████▍| 7202/7650 [11:45<00:46,  9.73it/s, loss=0.0263]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 7200] loss=0.0384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  95%|█████████▌| 7301/7650 [11:55<00:35,  9.96it/s, loss=0.0228]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 7300] loss=0.0194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  97%|█████████▋| 7401/7650 [12:06<00:24, 10.20it/s, loss=0.0460]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 7400] loss=0.0256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  98%|█████████▊| 7501/7650 [12:16<00:14, 10.09it/s, loss=0.0376]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 7500] loss=0.0162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  99%|█████████▉| 7601/7650 [12:26<00:04, 10.38it/s, loss=0.0160]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 7600] loss=0.0258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 7650/7650 [12:31<00:00, 10.18it/s, loss=0.0132]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  2] Loss=0.0311 | PSNR=26.91 dB | SSIM=0.7765\n",
      "  → New best model saved (PSNR 26.91)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100:   1%|▏         | 102/7650 [00:09<12:25, 10.13it/s, loss=0.0246]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch  100] loss=0.0490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100:   3%|▎         | 201/7650 [00:19<13:11,  9.41it/s, loss=0.0175]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch  200] loss=0.0249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100:   4%|▍         | 302/7650 [00:29<10:14, 11.95it/s, loss=0.0290]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch  300] loss=0.0300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100:   5%|▌         | 401/7650 [00:38<12:13,  9.88it/s, loss=0.0404]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch  400] loss=0.0245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100:   7%|▋         | 502/7650 [00:47<11:04, 10.76it/s, loss=0.0310]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch  500] loss=0.0205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100:   8%|▊         | 601/7650 [00:57<10:09, 11.56it/s, loss=0.0228]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch  600] loss=0.0209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100:   9%|▉         | 701/7650 [01:06<11:13, 10.32it/s, loss=0.0270]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch  700] loss=0.0204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100:  10%|█         | 802/7650 [01:16<10:48, 10.57it/s, loss=0.0348]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch  800] loss=0.0475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100:  12%|█▏        | 902/7650 [01:26<09:56, 11.31it/s, loss=0.0130]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch  900] loss=0.0241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100:  13%|█▎        | 1000/7650 [01:36<11:10,  9.91it/s, loss=0.0122]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 1000] loss=0.0292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100:  14%|█▍        | 1102/7650 [01:46<09:18, 11.73it/s, loss=0.0238]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 1100] loss=0.0373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100:  16%|█▌        | 1200/7650 [01:55<09:24, 11.42it/s, loss=0.0302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 1200] loss=0.0265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100:  17%|█▋        | 1300/7650 [02:06<14:19,  7.39it/s, loss=0.0400]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 1300] loss=0.0380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100:  18%|█▊        | 1401/7650 [02:17<09:23, 11.09it/s, loss=0.0428]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 1400] loss=0.0295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100:  20%|█▉        | 1500/7650 [02:29<12:07,  8.46it/s, loss=0.0349]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 1500] loss=0.0130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100:  20%|██        | 1561/7650 [02:35<10:07, 10.03it/s, loss=0.0277]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 213\u001b[0m\n\u001b[0;32m    210\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    211\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 213\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m batch_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    216\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# RAILSEM19 DEBLUR TRAINING – FIXED & MORE STABLE VERSION\n",
    "# ============================================================\n",
    "\n",
    "import cv2\n",
    "import cv2\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn               # ← fixed\n",
    "import torch.optim as optim         # ← use this instead of aliasing to nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "#  CONFIG\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[INFO] Using device: {DEVICE}\")\n",
    "\n",
    "DATA_DIR = Path(\"../data/datasets/RailSem19/jpgs/rs19_val\")\n",
    "assert DATA_DIR.exists(), f\"Folder not found: {DATA_DIR}\"\n",
    "\n",
    "OUT_DIR = Path(\"../outputs/models/deblur\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "IMG_SIZE    = 256\n",
    "BATCH_SIZE  = 1          # start small → increase later if stable\n",
    "LR          = 1e-4\n",
    "EPOCHS      = 100\n",
    "TRAIN_SPLIT = 0.9\n",
    "\n",
    "NUM_WORKERS    = 0       # 0 = safest, increase to 2–4 later if needed\n",
    "PIN_MEMORY     = torch.cuda.is_available()\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "#  MOTION BLUR KERNEL\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "def motion_blur_kernel(size, angle):\n",
    "    k = np.zeros((size, size), dtype=np.float32)\n",
    "    k[size//2, :] = 1.0\n",
    "    M = cv2.getRotationMatrix2D((size//2, size//2), angle, 1.0)\n",
    "    k = cv2.warpAffine(k, M, (size, size), flags=cv2.INTER_LINEAR)\n",
    "    return k / k.sum()\n",
    "\n",
    "def apply_motion_blur(img):\n",
    "    try:\n",
    "        kernel_size = random.choice([9, 11, 13, 15])\n",
    "        angle = random.uniform(-35, 35)\n",
    "        kernel = motion_blur_kernel(kernel_size, angle)\n",
    "        blurred = cv2.filter2D(img, -1, kernel)\n",
    "        return blurred\n",
    "    except Exception as e:\n",
    "        print(f\"Blur failed: {e}\")\n",
    "        return img  # fallback to sharp if blur crashes\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "#  DATASET\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "class RailSemDeblurDataset(Dataset):\n",
    "    def __init__(self, folder: Path):\n",
    "        self.files = sorted(folder.glob(\"*.jpg\"))\n",
    "        if not self.files:\n",
    "            raise ValueError(f\"No .jpg files found in {folder}\")\n",
    "        print(f\"[DATASET] Found {len(self.files)} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.files[idx]\n",
    "        img = cv2.imread(str(path))\n",
    "        if img is None:\n",
    "            raise IOError(f\"Cannot read image: {path}\")\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        sharp = img.astype(np.float32) / 255.0\n",
    "        blur  = apply_motion_blur(img).astype(np.float32) / 255.0\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(blur ).permute(2, 0, 1),\n",
    "            torch.from_numpy(sharp).permute(2, 0, 1)\n",
    "        )\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "#  MODEL\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1, bias=True)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1, bias=True)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv2(self.relu(self.conv1(x)))\n",
    "\n",
    "class DeblurNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.head   = nn.Conv2d(3,   64, 3, padding=1)\n",
    "        self.body   = nn.Sequential(*[ResBlock(64) for _ in range(12)])\n",
    "        self.tail   = nn.Conv2d(64,  3,  3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = torch.relu(self.head(x))\n",
    "        feat = self.body(feat)\n",
    "        out  = self.tail(feat)\n",
    "        return torch.clamp(out, 0.0, 1.0)\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "#  EVALUATION\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    psnrs, ssims = [], []\n",
    "    for blur, sharp in loader:\n",
    "        blur  = blur.to(DEVICE)\n",
    "        sharp = sharp.to(DEVICE)\n",
    "        pred = model(blur)\n",
    "\n",
    "        # take first image in batch for metric\n",
    "        p = pred[0].permute(1,2,0).cpu().numpy()\n",
    "        s = sharp[0].permute(1,2,0).cpu().numpy()\n",
    "\n",
    "        psnrs.append(psnr(s, p, data_range=1.0))\n",
    "        ssims.append(ssim(s, p, channel_axis=2, data_range=1.0))\n",
    "\n",
    "    return float(np.mean(psnrs)), float(np.mean(ssims))\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "#  MAIN\n",
    "# ────────────────────────────────────────────────\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ── Dataset + split ────────────────────────────────\n",
    "    full_dataset = RailSemDeblurDataset(DATA_DIR)\n",
    "\n",
    "    n_total = len(full_dataset)\n",
    "    n_train = int(n_total * TRAIN_SPLIT)\n",
    "    n_val   = n_total - n_train\n",
    "\n",
    "    train_ds, val_ds = random_split(full_dataset, [n_train, n_val])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "    )\n",
    "\n",
    "    print(f\"[INFO] Total: {n_total}  |  Train: {len(train_ds)}  |  Val: {len(val_ds)}\")\n",
    "    print(f\"[INFO] Batch size: {BATCH_SIZE}  |  Workers: {NUM_WORKERS}  |  Pin memory: {PIN_MEMORY}\")\n",
    "\n",
    "    # ── Quick test: can we load one batch? ─────────────\n",
    "    print(\"\\nTesting first batch ... \", end=\"\")\n",
    "    try:\n",
    "        test_blur, test_sharp = next(iter(train_loader))\n",
    "        print(\"OK\")\n",
    "        print(f\"   shapes → blur: {test_blur.shape}   sharp: {test_sharp.shape}\")\n",
    "    except Exception as e:\n",
    "        print(\"FAILED\")\n",
    "        print(e)\n",
    "        exit(1)\n",
    "\n",
    "    # ── Model + Optimizer + Loss ───────────────────────\n",
    "    model = DeblurNet().to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_fn = nn.L1Loss()\n",
    "\n",
    "    best_psnr = -1.0\n",
    "    best_path = OUT_DIR / \"best_model_deblur.pth\"\n",
    "\n",
    "    # ── Training loop ──────────────────────────────────\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        batch_count = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "        for blur, sharp in pbar:\n",
    "            blur  = blur.to(DEVICE)\n",
    "            sharp = sharp.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(blur)\n",
    "            loss = loss_fn(pred, sharp)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "            # occasional print\n",
    "            if batch_count % 100 == 0:\n",
    "                print(f\"  [batch {batch_count:4d}] loss={loss.item():.4f}\")\n",
    "\n",
    "        avg_loss = total_loss / batch_count\n",
    "\n",
    "        # ── Validation ────────────────────────────────\n",
    "        val_psnr, val_ssim = evaluate(model, val_loader)\n",
    "\n",
    "        print(f\"[Epoch {epoch:2d}] \"\n",
    "              f\"Loss={avg_loss:.4f} | \"\n",
    "              f\"PSNR={val_psnr:.2f} dB | \"\n",
    "              f\"SSIM={val_ssim:.4f}\")\n",
    "\n",
    "        # Save every epoch\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"loss\": avg_loss,\n",
    "                \"val_psnr\": val_psnr,\n",
    "                \"val_ssim\": val_ssim,\n",
    "            },\n",
    "            OUT_DIR / f\"epoch_{epoch:03d}.pth\"\n",
    "        )\n",
    "\n",
    "        # Save best model\n",
    "        if val_psnr > best_psnr:\n",
    "            best_psnr = val_psnr\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print(f\"  → New best model saved (PSNR {val_psnr:.2f})\")\n",
    "\n",
    "    print(\"\\n\" + \"═\" * 60)\n",
    "    print(\"Training finished. Best PSNR:\", f\"{best_psnr:.2f}\")\n",
    "    print(\"Best model saved at:\", best_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc087943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Device: cuda\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "No images found!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 69\u001b[0m\n\u001b[0;32m     65\u001b[0m         blur  \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(blur  \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m blur, sharp\n\u001b[1;32m---> 69\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mRailBlurDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_ROOT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Training samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 50\u001b[0m, in \u001b[0;36mRailBlurDataset.__init__\u001b[1;34m(self, root)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, root):\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(root\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 50\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo images found!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: No images found!"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# RAILWAY DEBLUR TRAINING PIPELINE (SYNTHETIC MOTION BLUR)\n",
    "# ============================================================\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =====================\n",
    "# CONFIG\n",
    "# =====================\n",
    "DATA_ROOT = Path(\"../data/datasets/RailSem19/JPEGImages\")\n",
    "SAVE_DIR  = Path(\"../outputs/models/deblur\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 30\n",
    "LR = 1e-4\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"[INFO] Device:\", DEVICE)\n",
    "\n",
    "# =====================\n",
    "# SYNTHETIC BLUR\n",
    "# =====================\n",
    "def apply_motion_blur(img, max_kernel=25):\n",
    "    k = np.random.randint(7, max_kernel)\n",
    "    kernel = np.zeros((k, k))\n",
    "    kernel[k // 2, :] = np.ones(k)\n",
    "    kernel /= k\n",
    "    return cv2.filter2D(img, -1, kernel)\n",
    "\n",
    "def add_night_noise(img):\n",
    "    noise = np.random.normal(0, 8, img.shape)\n",
    "    noisy = img + noise\n",
    "    return np.clip(noisy, 0, 255).astype(np.uint8)\n",
    "\n",
    "# =====================\n",
    "# DATASET\n",
    "# =====================\n",
    "class RailBlurDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.images = list(root.glob(\"*.jpg\"))\n",
    "        assert len(self.images) > 0, \"No images found!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(str(self.images[idx]))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "        sharp = img.copy()\n",
    "        blur  = apply_motion_blur(img)\n",
    "        blur  = add_night_noise(blur)\n",
    "\n",
    "        sharp = torch.from_numpy(sharp / 255.0).permute(2,0,1).float()\n",
    "        blur  = torch.from_numpy(blur  / 255.0).permute(2,0,1).float()\n",
    "\n",
    "        return blur, sharp\n",
    "\n",
    "dataset = RailBlurDataset(DATA_ROOT)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "print(f\"[INFO] Training samples: {len(dataset)}\")\n",
    "\n",
    "# =====================\n",
    "# MODEL\n",
    "# =====================\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(c, c, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(c, c, 3, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv2(self.relu(self.conv1(x)))\n",
    "\n",
    "class DeblurGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.head = nn.Conv2d(3, 64, 7, padding=3)\n",
    "        self.body = nn.Sequential(*[ResBlock(64) for _ in range(9)])\n",
    "        self.tail = nn.Conv2d(64, 3, 7, padding=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.head(x))\n",
    "        x = self.body(x)\n",
    "        return torch.sigmoid(self.tail(x))\n",
    "\n",
    "model = DeblurGenerator().to(DEVICE)\n",
    "\n",
    "# =====================\n",
    "# TRAINING SETUP\n",
    "# =====================\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
    "\n",
    "# =====================\n",
    "# TRAIN LOOP\n",
    "# =====================\n",
    "print(\"[INFO] Starting training...\")\n",
    "\n",
    "best_loss = 1e9\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for blur, sharp in tqdm(loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        blur  = blur.to(DEVICE)\n",
    "        sharp = sharp.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE.type == \"cuda\")):\n",
    "            pred = model(blur)\n",
    "            loss = criterion(pred, sharp)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(loader)\n",
    "    print(f\"[Epoch {epoch+1}] L1 Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(\n",
    "            {\"model_state\": model.state_dict()},\n",
    "            SAVE_DIR / \"best_model_deblur.pth\"\n",
    "        )\n",
    "        print(\"✅ Saved best model\")\n",
    "\n",
    "print(\"🎉 TRAINING COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e211e871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Device: cuda\n",
      "[INFO] Loaded 8500 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Swayam\\AppData\\Local\\Temp\\ipykernel_18764\\3121918670.py:120: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == \"cuda\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:   0%|          | 0/2125 [00:00<?, ?it/s]C:\\Users\\Swayam\\AppData\\Local\\Temp\\ipykernel_18764\\3121918670.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE.type == \"cuda\")):\n",
      "Epoch 1/30: 100%|██████████| 2125/2125 [10:16<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] L1 Loss: 0.0505\n",
      "✅ Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|██████████| 2125/2125 [10:00<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] L1 Loss: 0.0460\n",
      "✅ Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|██████████| 2125/2125 [09:44<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] L1 Loss: 0.0448\n",
      "✅ Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 100%|██████████| 2125/2125 [11:11<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] L1 Loss: 0.0438\n",
      "✅ Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: 100%|██████████| 2125/2125 [08:47<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] L1 Loss: 0.0433\n",
      "✅ Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30: 100%|██████████| 2125/2125 [08:36<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] L1 Loss: 0.0426\n",
      "✅ Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30: 100%|██████████| 2125/2125 [10:04<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] L1 Loss: 0.0422\n",
      "✅ Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30: 100%|██████████| 2125/2125 [08:57<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] L1 Loss: 0.0418\n",
      "✅ Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30: 100%|██████████| 2125/2125 [08:39<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] L1 Loss: 0.0414\n",
      "✅ Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30: 100%|██████████| 2125/2125 [09:42<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] L1 Loss: 0.0411\n",
      "✅ Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30: 100%|██████████| 2125/2125 [09:02<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] L1 Loss: 0.0409\n",
      "✅ Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30: 100%|██████████| 2125/2125 [08:31<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] L1 Loss: 0.0406\n",
      "✅ Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30: 100%|██████████| 2125/2125 [08:22<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] L1 Loss: 0.0404\n",
      "✅ Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30: 100%|██████████| 2125/2125 [08:26<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] L1 Loss: 0.0400\n",
      "✅ Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30: 100%|██████████| 2125/2125 [08:21<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] L1 Loss: 0.0400\n",
      "✅ Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30: 100%|██████████| 2125/2125 [08:21<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16] L1 Loss: 0.0396\n",
      "✅ Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30: 100%|██████████| 2125/2125 [08:21<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17] L1 Loss: 0.0395\n",
      "✅ Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30: 100%|██████████| 2125/2125 [08:21<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18] L1 Loss: 0.0393\n",
      "✅ Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30: 100%|██████████| 2125/2125 [09:10<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19] L1 Loss: 0.0392\n",
      "✅ Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30: 100%|██████████| 2125/2125 [09:46<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20] L1 Loss: 0.0390\n",
      "✅ Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30: 100%|██████████| 2125/2125 [12:31<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 21] L1 Loss: 0.0390\n",
      "✅ Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30: 100%|██████████| 2125/2125 [12:41<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 22] L1 Loss: 0.0389\n",
      "✅ Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30: 100%|██████████| 2125/2125 [11:00<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23] L1 Loss: 0.0388\n",
      "✅ Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30: 100%|██████████| 2125/2125 [10:39<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 24] L1 Loss: 0.0386\n",
      "✅ Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30: 100%|██████████| 2125/2125 [10:32<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 25] L1 Loss: 0.0385\n",
      "✅ Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30: 100%|██████████| 2125/2125 [14:03<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 26] L1 Loss: 0.0384\n",
      "✅ Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30: 100%|██████████| 2125/2125 [14:18<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 27] L1 Loss: 0.0383\n",
      "✅ Best model updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30:  26%|██▌       | 553/2125 [04:05<11:38,  2.25it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 140\u001b[0m\n\u001b[0;32m    137\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(pred, sharp)\n\u001b[0;32m    139\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 140\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m    143\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Swayam\\OneDrive\\Desktop\\Adani - RailVision\\gpuenv\\lib\\site-packages\\torch\\amp\\grad_scaler.py:457\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    455\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 457\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    459\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\Swayam\\OneDrive\\Desktop\\Adani - RailVision\\gpuenv\\lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\Swayam\\OneDrive\\Desktop\\Adani - RailVision\\gpuenv\\lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# RAILWAY DEBLUR TRAINING (RailSem19 rs19_val)\n",
    "# ============================================================\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# =====================\n",
    "# CONFIG\n",
    "# =====================\n",
    "DATA_DIR = Path(\"../data/datasets/RailSem19/jpgs/rs19_val\")\n",
    "SAVE_DIR = Path(\"../outputs/models/deblur\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 30\n",
    "LR = 1e-4\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"[INFO] Device:\", DEVICE)\n",
    "\n",
    "# =====================\n",
    "# SYNTHETIC BLUR (PROPER)\n",
    "# =====================\n",
    "def motion_blur(img):\n",
    "    k = random.choice([9, 11, 15, 21])\n",
    "    angle = random.uniform(-20, 20)  # train motion variation\n",
    "\n",
    "    kernel = np.zeros((k, k))\n",
    "    kernel[k // 2, :] = np.ones(k)\n",
    "    kernel /= k\n",
    "\n",
    "    M = cv2.getRotationMatrix2D((k//2, k//2), angle, 1)\n",
    "    kernel = cv2.warpAffine(kernel, M, (k, k))\n",
    "\n",
    "    return cv2.filter2D(img, -1, kernel)\n",
    "\n",
    "def night_noise(img):\n",
    "    noise = np.random.normal(0, 10, img.shape)\n",
    "    noisy = img + noise\n",
    "    return np.clip(noisy, 0, 255).astype(np.uint8)\n",
    "\n",
    "# =====================\n",
    "# DATASET (FIXED)\n",
    "# =====================\n",
    "class RailSemBlurDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.images = list(root.rglob(\"*.jpg\"))\n",
    "        assert len(self.images) > 0, \"❌ No images found\"\n",
    "        print(f\"[INFO] Loaded {len(self.images)} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(str(self.images[idx]))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "        sharp = img.copy()\n",
    "        blur = motion_blur(img)\n",
    "        blur = night_noise(blur)\n",
    "\n",
    "        sharp = torch.from_numpy(sharp / 255.0).permute(2,0,1).float()\n",
    "        blur  = torch.from_numpy(blur  / 255.0).permute(2,0,1).float()\n",
    "\n",
    "        return blur, sharp\n",
    "\n",
    "dataset = RailSemBlurDataset(DATA_DIR)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,   # 🔥 CRITICAL FIX\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "# =====================\n",
    "# MODEL\n",
    "# =====================\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(c, c, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(c, c, 3, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv2(self.relu(self.conv1(x)))\n",
    "\n",
    "class DeblurGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.head = nn.Conv2d(3, 64, 7, padding=3)\n",
    "        self.body = nn.Sequential(*[ResBlock(64) for _ in range(9)])\n",
    "        self.tail = nn.Conv2d(64, 3, 7, padding=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.head(x))\n",
    "        x = self.body(x)\n",
    "        return torch.sigmoid(self.tail(x))\n",
    "\n",
    "model = DeblurGenerator().to(DEVICE)\n",
    "\n",
    "# =====================\n",
    "# TRAINING\n",
    "# =====================\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "print(\"[INFO] Training started\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for blur, sharp in tqdm(loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        blur = blur.to(DEVICE)\n",
    "        sharp = sharp.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE.type == \"cuda\")):\n",
    "            pred = model(blur)\n",
    "            loss = criterion(pred, sharp)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(loader)\n",
    "    print(f\"[Epoch {epoch+1}] L1 Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"loss\": epoch_loss,\n",
    "            \"model_state\": model.state_dict()\n",
    "        },\n",
    "        SAVE_DIR / f\"epoch_{epoch+1}.pth\"\n",
    "    )\n",
    "\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(\n",
    "            {\"model_state\": model.state_dict()},\n",
    "            SAVE_DIR / \"best_model_deblur.pth\"\n",
    "        )\n",
    "        print(\"✅ Best model updated\")\n",
    "\n",
    "print(\"🎉 TRAINING COMPLETE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5636af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuenv (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
