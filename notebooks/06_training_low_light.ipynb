{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ee002ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb9045ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"[INFO] Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c175d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# DATASET PATHS\n",
    "# =====================\n",
    "LOL_DIR = Path(\"../data/datasets/LOL\")\n",
    "\n",
    "TRAIN_LOW = LOL_DIR / \"our485/low\"\n",
    "TRAIN_HIGH = LOL_DIR / \"our485/high\"\n",
    "\n",
    "VAL_LOW = LOL_DIR / \"eval15/low\"\n",
    "VAL_HIGH = LOL_DIR / \"eval15/high\"\n",
    "\n",
    "# =====================\n",
    "# OUTPUT PATHS\n",
    "# =====================\n",
    "MODEL_SAVE_DIR = Path(\"../outputs/models/low_light\")\n",
    "MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =====================\n",
    "# TRAINING PARAMS\n",
    "# =====================\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 250\n",
    "LR = 1e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "867337f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    img = cv2.imread(str(path))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img.astype(np.float32) / 255.0\n",
    "\n",
    "def to_tensor(img):\n",
    "    return torch.from_numpy(img).permute(2, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49674d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class LOLDataset(Dataset):\n",
    "    def __init__(self, low_dir, high_dir):\n",
    "        self.low_imgs = sorted(\n",
    "            list(low_dir.glob(\"*.png\")) + list(low_dir.glob(\"*.jpg\"))\n",
    "        )\n",
    "        self.high_dir = high_dir\n",
    "\n",
    "        if len(self.low_imgs) == 0:\n",
    "            raise RuntimeError(f\"No images found in {low_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.low_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        low_path = self.low_imgs[idx]\n",
    "        high_path = self.high_dir / low_path.name\n",
    "\n",
    "        low = to_tensor(read_image(low_path))\n",
    "        high = to_tensor(read_image(high_path))\n",
    "\n",
    "        return low, high\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bdf2a8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No images found in ..\\data\\datasets\\LOL\\our485\\low",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_dataset = \u001b[43mLOLDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_LOW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRAIN_HIGH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m val_dataset = LOLDataset(VAL_LOW, VAL_HIGH)\n\u001b[32m      4\u001b[39m train_loader = DataLoader(\n\u001b[32m      5\u001b[39m     train_dataset,\n\u001b[32m      6\u001b[39m     batch_size=BATCH_SIZE,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     pin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     10\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mLOLDataset.__init__\u001b[39m\u001b[34m(self, low_dir, high_dir)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mself\u001b[39m.high_dir = high_dir\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.low_imgs) == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo images found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlow_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: No images found in ..\\data\\datasets\\LOL\\our485\\low"
     ]
    }
   ],
   "source": [
    "train_dataset = LOLDataset(TRAIN_LOW, TRAIN_HIGH)\n",
    "val_dataset = LOLDataset(VAL_LOW, VAL_HIGH)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6944545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroDCE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.conv7 = nn.Conv2d(32, 24, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.relu(self.conv1(x))\n",
    "        x2 = self.relu(self.conv2(x1))\n",
    "        x3 = self.relu(self.conv3(x2))\n",
    "        x4 = self.relu(self.conv4(x3))\n",
    "        x5 = self.relu(self.conv5(x4))\n",
    "        x6 = self.relu(self.conv6(x5))\n",
    "        curves = torch.tanh(self.conv7(x6))\n",
    "        return curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df16ed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def illumination_smoothness_loss(curves):\n",
    "    return torch.mean(torch.abs(curves[:, :, :-1, :] - curves[:, :, 1:, :])) + \\\n",
    "           torch.mean(torch.abs(curves[:, :, :, :-1] - curves[:, :, :, 1:]))\n",
    "\n",
    "def exposure_loss(img, target=0.6):\n",
    "    gray = torch.mean(img, dim=1, keepdim=True)\n",
    "    return torch.mean((gray - target) ** 2)\n",
    "\n",
    "def color_constancy_loss(img):\n",
    "    mean_rgb = torch.mean(img, dim=[2,3])\n",
    "    r, g, b = mean_rgb[:,0], mean_rgb[:,1], mean_rgb[:,2]\n",
    "    return torch.mean((r-g)**2 + (r-b)**2 + (g-b)**2)\n",
    "\n",
    "def spatial_consistency_loss(img_low, img_high):\n",
    "    return torch.mean(torch.abs(img_low - img_high))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef4b1b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_curves(img, curves, n_iters=8):\n",
    "    enhanced = img\n",
    "    for i in range(n_iters):\n",
    "        r = curves[:, i*3:(i+1)*3, :, :]\n",
    "        enhanced = enhanced + r * (enhanced**2 - enhanced)\n",
    "    return enhanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0de0e6bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m train_losses = []\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[43mmodel\u001b[49m.train()\n\u001b[32m      6\u001b[39m     epoch_loss = \u001b[32m0\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m low, high \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "best_loss = float(\"inf\")\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for low, high in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "        low = low.to(DEVICE)\n",
    "        high = high.to(DEVICE)\n",
    "\n",
    "        curves = model(low)\n",
    "        enhanced = apply_curves(low, curves)\n",
    "\n",
    "        loss = (\n",
    "            exposure_loss(enhanced) +\n",
    "            color_constancy_loss(enhanced) +\n",
    "            illumination_smoothness_loss(curves) +\n",
    "            spatial_consistency_loss(enhanced, high)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(train_loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    # ---- Save epoch ----\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"loss\": epoch_loss\n",
    "        },\n",
    "        MODEL_SAVE_DIR / f\"epoch_{epoch:03d}.pth\"\n",
    "    )\n",
    "\n",
    "    # ---- Save best ----\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"best_loss\": best_loss\n",
    "            },\n",
    "            MODEL_SAVE_DIR / \"best_model.pth\"\n",
    "        )\n",
    "\n",
    "    print(f\"[Epoch {epoch}] Loss = {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149f235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(train_losses)\n",
    "plt.title(\"Zero-DCE Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuenv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
